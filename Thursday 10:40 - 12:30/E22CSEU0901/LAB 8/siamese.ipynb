{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize\n",
    "x_train, x_test = np.expand_dims(x_train, -1), np.expand_dims(x_test, -1)\n",
    "\n",
    "\n",
    "\n",
    "def build_siamese_model():\n",
    "    input_layer = layers.Input(shape=(28, 28, 1))\n",
    "    x = layers.Conv2D(64, (3,3), activation='relu')(input_layer)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    return models.Model(input_layer, x)\n",
    "\n",
    "siamese_model = build_siamese_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Layer \"functional_1\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'data:0' shape=(None, 28, 28, 1) dtype=float32>, <tf.Tensor 'data_1:0' shape=(None, 28, 28, 1) dtype=float32>]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Positive pair\u001b[39;00m\n\u001b[0;32m     15\u001b[0m pairs, labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(pairs), np\u001b[38;5;241m.\u001b[39marray(labels)\n\u001b[1;32m---> 16\u001b[0m \u001b[43msiamese_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Evaluate on new pairs\u001b[39;00m\n\u001b[0;32m     19\u001b[0m test_pairs, test_labels \u001b[38;5;241m=\u001b[39m sample_episode(x_test, y_test, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\anand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\anand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:160\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    158\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mflatten(inputs)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_spec):\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expects \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(input_spec)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input(s),\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but it received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input tensors. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputs received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    164\u001b[0m     )\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_index, (x, spec) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(inputs, input_spec)):\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Layer \"functional_1\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'data:0' shape=(None, 28, 28, 1) dtype=float32>, <tf.Tensor 'data_1:0' shape=(None, 28, 28, 1) dtype=float32>]"
     ]
    }
   ],
   "source": [
    "def contrastive_loss(y_true, y_pred):\n",
    "    margin = 1.0\n",
    "    return tf.reduce_mean(y_true * tf.square(y_pred) + (1 - y_true) * tf.square(tf.maximum(margin - y_pred, 0)))\n",
    "\n",
    "siamese_model.compile(loss=contrastive_loss, optimizer='adam')\n",
    "\n",
    "pairs, labels = [], []\n",
    "for _ in range(10000):\n",
    "    cls = np.random.choice(np.unique(y_train))\n",
    "    idx = np.where(y_train == cls)[0]\n",
    "    np.random.shuffle(idx)\n",
    "    pairs.append([x_train[idx[0]], x_train[idx[1]]])\n",
    "    labels.append(1)  # Positive pair\n",
    "\n",
    "pairs, labels = np.array(pairs), np.array(labels)\n",
    "siamese_model.fit([pairs[:, 0], pairs[:, 1]], labels, epochs=5)\n",
    "\n",
    "# Evaluate on new pairs\n",
    "test_pairs, test_labels = sample_episode(x_test, y_test, 2, 1, 1)\n",
    "test_preds = siamese_model.predict([test_pairs[:, 0], test_pairs[:, 1]])\n",
    "print(\"Siamese Network Results:\")\n",
    "print(classification_report(test_labels.flatten(), (test_preds > 0.5).astype(int).flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\anand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Epoch 1/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - accuracy: 0.5103 - loss: 6.8001\n",
      "Epoch 2/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 0.4360 - loss: 8.9909\n",
      "Epoch 3/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 0.5178 - loss: 7.6876\n",
      "Epoch 4/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 0.5040 - loss: 7.9071\n",
      "Epoch 5/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 0.5103 - loss: 7.8074\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Siamese Network Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      1.00      0.60        85\n",
      "           1       0.00      0.00      0.00       115\n",
      "\n",
      "    accuracy                           0.42       200\n",
      "   macro avg       0.21      0.50      0.30       200\n",
      "weighted avg       0.18      0.42      0.25       200\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\anand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\anand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 61ms/step - accuracy: 0.0416 - loss: 0.6997\n",
      "Epoch 2/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.0234 - loss: 0.5273\n",
      "Epoch 3/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 0.0090 - loss: 0.2321\n",
      "Epoch 4/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.2643 - loss: 0.1125\n",
      "Epoch 5/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 0.2073 - loss: 0.0468\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Matching Network Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.81      0.73        85\n",
      "           1       0.83      0.70      0.76       115\n",
      "\n",
      "    accuracy                           0.74       200\n",
      "   macro avg       0.75      0.75      0.74       200\n",
      "weighted avg       0.76      0.74      0.75       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize to [0,1]\n",
    "\n",
    "# Reshape to (28,28,1)\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Get classes\n",
    "num_classes = 10\n",
    "train_classes = {i: x_train[y_train == i] for i in range(num_classes)}\n",
    "test_classes = {i: x_test[y_test == i] for i in range(num_classes)}\n",
    "\n",
    "\n",
    "### **SIAMESE NETWORK**\n",
    "def build_siamese_network(input_shape):\n",
    "    input_1 = Input(input_shape)\n",
    "    input_2 = Input(input_shape)\n",
    "\n",
    "    def base_network(x):\n",
    "        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        return x\n",
    "\n",
    "    embedding_1 = base_network(input_1)\n",
    "    embedding_2 = base_network(input_2)\n",
    "\n",
    "    # Distance function\n",
    "    def euclidean_distance(vectors):\n",
    "        return K.sqrt(K.sum(K.square(vectors[0] - vectors[1]), axis=1, keepdims=True))\n",
    "\n",
    "    distance = Lambda(euclidean_distance)([embedding_1, embedding_2])\n",
    "\n",
    "    model = Model(inputs=[input_1, input_2], outputs=distance)\n",
    "    return model\n",
    "\n",
    "siamese_model = build_siamese_network((28, 28, 1))\n",
    "siamese_model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# Sample pairs\n",
    "def sample_pairs(classes, num_pairs=500):\n",
    "    pairs, labels = [], []\n",
    "    \n",
    "    for _ in range(num_pairs):\n",
    "        label = random.choice([0, 1])\n",
    "        cls = random.choice(list(classes.keys()))\n",
    "        if label == 1:\n",
    "            pair = np.random.choice(len(classes[cls]), 2, replace=False)\n",
    "            pairs.append([classes[cls][pair[0]], classes[cls][pair[1]]])\n",
    "        else:\n",
    "            cls1, cls2 = np.random.choice(list(classes.keys()), 2, replace=False)\n",
    "            pairs.append([random.choice(classes[cls1]), random.choice(classes[cls2])])\n",
    "\n",
    "        labels.append(label)\n",
    "\n",
    "    return np.array(pairs), np.array(labels)\n",
    "\n",
    "train_pairs, train_labels = sample_pairs(train_classes, 500)\n",
    "test_pairs, test_labels = sample_pairs(test_classes, 200)\n",
    "\n",
    "# Train Siamese Network\n",
    "siamese_model.fit([train_pairs[:, 0], train_pairs[:, 1]], train_labels, batch_size=16, epochs=5)\n",
    "\n",
    "# Evaluate\n",
    "pred_distances = siamese_model.predict([test_pairs[:, 0], test_pairs[:, 1]])\n",
    "predictions = (pred_distances < 0.5).astype(int)\n",
    "print(\"Siamese Network Results:\")\n",
    "print(classification_report(test_labels, predictions))\n",
    "\n",
    "### **MATCHING NETWORK**\n",
    "def cosine_similarity(vectors):\n",
    "    x, y = vectors\n",
    "    return K.sum(x * y, axis=1) / (K.sqrt(K.sum(K.square(x), axis=1)) * K.sqrt(K.sum(K.square(y), axis=1)))\n",
    "\n",
    "def build_matching_network(input_shape):\n",
    "    input_1 = Input(input_shape)\n",
    "    input_2 = Input(input_shape)\n",
    "\n",
    "    def base_network(x):\n",
    "        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        return x\n",
    "\n",
    "    embedding_1 = base_network(input_1)\n",
    "    embedding_2 = base_network(input_2)\n",
    "\n",
    "    similarity = Lambda(cosine_similarity)([embedding_1, embedding_2])\n",
    "    model = Model(inputs=[input_1, input_2], outputs=similarity)\n",
    "    return model\n",
    "\n",
    "matching_model = build_matching_network((28, 28, 1))\n",
    "matching_model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# Train Matching Network\n",
    "matching_model.fit([train_pairs[:, 0], train_pairs[:, 1]], train_labels, batch_size=16, epochs=5)\n",
    "\n",
    "# Evaluate\n",
    "pred_similarities = matching_model.predict([test_pairs[:, 0], test_pairs[:, 1]])\n",
    "predictions = (pred_similarities > 0.5).astype(int)\n",
    "print(\"Matching Network Results:\")\n",
    "print(classification_report(test_labels, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototypical Network Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.60      0.63        10\n",
      "           1       0.60      0.30      0.40        10\n",
      "           2       0.53      0.90      0.67        10\n",
      "           3       0.62      0.50      0.56        10\n",
      "           4       0.64      0.70      0.67        10\n",
      "\n",
      "    accuracy                           0.60        50\n",
      "   macro avg       0.61      0.60      0.58        50\n",
      "weighted avg       0.61      0.60      0.58        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize to [0,1]\n",
    "\n",
    "# Reshape to (28,28,1)\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Group images by class\n",
    "num_classes = 10\n",
    "train_classes = {i: x_train[y_train == i] for i in range(num_classes)}\n",
    "test_classes = {i: x_test[y_test == i] for i in range(num_classes)}\n",
    "\n",
    "# Sample few-shot episode\n",
    "def sample_episode(classes, n_classes=5, k_shot=5, q_query=10):\n",
    "    selected_classes = np.random.choice(list(classes.keys()), n_classes, replace=False)\n",
    "    support_set, query_set, labels = [], [], []\n",
    "    \n",
    "    for label, cls in enumerate(selected_classes):\n",
    "        samples = np.random.choice(len(classes[cls]), k_shot + q_query, replace=False)\n",
    "        support, query = samples[:k_shot], samples[k_shot:]\n",
    "        support_set.append(classes[cls][support])\n",
    "        query_set.append(classes[cls][query])\n",
    "        labels.append([label] * q_query)\n",
    "\n",
    "    support_set = np.array(support_set)  # Shape: (N, k, 28, 28, 1)\n",
    "    query_set = np.array(query_set)      # Shape: (N, q, 28, 28, 1)\n",
    "    labels = np.array(labels).flatten()  # Shape: (N * q,)\n",
    "\n",
    "    return support_set, query_set, labels\n",
    "\n",
    "# Compute class prototypes (mean embedding)\n",
    "def compute_prototypes(support_set):\n",
    "    return np.mean(support_set, axis=1)  # Shape: (N, 28, 28, 1)\n",
    "\n",
    "# Classify query samples based on Euclidean distance\n",
    "def classify_query(query_set, prototypes):\n",
    "    N, Q, H, W, C = query_set.shape  # N = num classes, Q = query samples/class\n",
    "\n",
    "    # Reshape for broadcasting\n",
    "    query_set = query_set.reshape(N * Q, H * W * C)  # (N*Q, 28*28*1)\n",
    "    prototypes = prototypes.reshape(N, H * W * C)    # (N, 28*28*1)\n",
    "\n",
    "    # Compute Euclidean distances\n",
    "    distances = np.linalg.norm(query_set[:, np.newaxis, :] - prototypes, axis=2)  # (N*Q, N)\n",
    "    \n",
    "    return np.argmin(distances, axis=1)  # (N*Q,)\n",
    "\n",
    "# Sample an episode\n",
    "support, query, true_labels = sample_episode(train_classes)\n",
    "\n",
    "# Compute prototypes & classify\n",
    "prototypes = compute_prototypes(support)\n",
    "pred_labels = classify_query(query, prototypes)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Prototypical Network Results:\")\n",
    "print(classification_report(true_labels, pred_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
