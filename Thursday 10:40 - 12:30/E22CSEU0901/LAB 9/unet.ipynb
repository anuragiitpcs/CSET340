{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32ace1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🐍 Step 1: Import Libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f18a786f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Failed to construct dataset \"oxford_iiit_pet\", builder_kwargs \"{'version': '3.*.*', 'data_dir': None}\": Dataset oxford_iiit_pet cannot be loaded at version 3.*.*, only: 4.0.0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 📥 Step 2: Load Dataset\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Oxford-IIIT Pet comes with pre-labeled segmentation masks\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m dataset, info \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moxford_iiit_pet:3.*.*\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m test_ds \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\anand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:176\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[1;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[1;32mc:\\Users\\anand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_datasets\\core\\load.py:660\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs, file_format)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;129m@tfds_logging\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m    523\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    539\u001b[0m ):\n\u001b[0;32m    540\u001b[0m   \u001b[38;5;66;03m# pylint: disable=line-too-long\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Loads the named dataset into a `tf.data.Dataset`.\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \n\u001b[0;32m    543\u001b[0m \u001b[38;5;124;03m  `tfds.load` is a convenience method that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;124;03m      Split-specific information is available in `ds_info.splits`.\u001b[39;00m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m  \u001b[38;5;66;03m# fmt: skip\u001b[39;00m\n\u001b[1;32m--> 660\u001b[0m   dbuilder \u001b[38;5;241m=\u001b[39m \u001b[43m_fetch_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    661\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtry_gcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_gcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    665\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    666\u001b[0m   _download_and_prepare_builder(dbuilder, download, download_and_prepare_kwargs)\n\u001b[0;32m    668\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m as_dataset_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\anand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_datasets\\core\\load.py:495\u001b[0m, in \u001b[0;36m_fetch_builder\u001b[1;34m(name, data_dir, builder_kwargs, try_gcs, file_format)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    494\u001b[0m   builder_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_format\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m file_format\n\u001b[1;32m--> 495\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_gcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_gcs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:176\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[1;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[1;32mc:\\Users\\anand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_datasets\\core\\load.py:218\u001b[0m, in \u001b[0;36mbuilder\u001b[1;34m(name, try_gcs, **builder_kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m:\n\u001b[0;32m    215\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m py_utils\u001b[38;5;241m.\u001b[39mtry_reraise(\n\u001b[0;32m    216\u001b[0m       prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to construct \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mget_dataset_repr()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    217\u001b[0m   ):\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pytype: disable=not-instantiable\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;66;03m# If neither the code nor the files are found, raise DatasetNotFoundError\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m not_found_error\n",
      "File \u001b[1;32mc:\\Users\\anand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:296\u001b[0m, in \u001b[0;36mbuilder_init.<locals>.decorator\u001b[1;34m(function, dsbuilder, args, kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m _thread_id_to_builder_init_count[metadata\u001b[38;5;241m.\u001b[39mthread_id] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 296\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[1;32mc:\\Users\\anand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:1502\u001b[0m, in \u001b[0;36mFileReaderBuilder.__init__\u001b[1;34m(self, file_format, **kwargs)\u001b[0m\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;129m@tfds_logging\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder_init()\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m   1487\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1490\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1491\u001b[0m ):\n\u001b[0;32m   1492\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Initializes an instance of FileReaderBuilder.\u001b[39;00m\n\u001b[0;32m   1493\u001b[0m \n\u001b[0;32m   1494\u001b[0m \u001b[38;5;124;03m  Callers must pass arguments as keyword arguments.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m \u001b[38;5;124;03m    **kwargs: Arguments passed to `DatasetBuilder`.\u001b[39;00m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1502\u001b[0m   \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1503\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mset_file_format(file_format)\n",
      "File \u001b[1;32mc:\\Users\\anand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:296\u001b[0m, in \u001b[0;36mbuilder_init.<locals>.decorator\u001b[1;34m(function, dsbuilder, args, kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m _thread_id_to_builder_init_count[metadata\u001b[38;5;241m.\u001b[39mthread_id] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 296\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[1;32mc:\\Users\\anand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:289\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[1;34m(self, data_dir, config, version)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_builder_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_builder_config(config, version\u001b[38;5;241m=\u001b[39mversion)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# Extract code version (VERSION or config)\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pick_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# Compute the base directory (for download) and dataset/version directory.\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_dir_root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_data_dir(data_dir)\n",
      "File \u001b[1;32mc:\\Users\\anand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:403\u001b[0m, in \u001b[0;36mDatasetBuilder._pick_version\u001b[1;34m(self, requested_version)\u001b[0m\n\u001b[0;32m    399\u001b[0m available_versions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mversions]\n\u001b[0;32m    400\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m cannot be loaded at version \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, only: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, requested_version, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(available_versions)\n\u001b[0;32m    402\u001b[0m )\n\u001b[1;32m--> 403\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(msg)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Failed to construct dataset \"oxford_iiit_pet\", builder_kwargs \"{'version': '3.*.*', 'data_dir': None}\": Dataset oxford_iiit_pet cannot be loaded at version 3.*.*, only: 4.0.0."
     ]
    }
   ],
   "source": [
    "# 📥 Step 2: Load Dataset\n",
    "# Oxford-IIIT Pet comes with pre-labeled segmentation masks\n",
    "dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)\n",
    "train_ds = dataset['train']\n",
    "test_ds = dataset['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a09207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧹 Step 3: Preprocess Data\n",
    "IMG_SIZE = 128\n",
    "\n",
    "def normalize_img(datapoint):\n",
    "    image = tf.image.resize(datapoint['image'], (IMG_SIZE, IMG_SIZE)) / 255.0\n",
    "    mask = tf.image.resize(datapoint['segmentation_mask'], (IMG_SIZE, IMG_SIZE))\n",
    "    mask = tf.where(mask == 2, 1, mask)  # pet (1), background (0)\n",
    "    return image, tf.cast(mask, tf.float32)\n",
    "\n",
    "train_data = train_ds.map(normalize_img).cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_data = test_ds.map(normalize_img).cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36808351",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'IMG_SIZE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 🧠 Step 4: Build U-Net Model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21munet_model\u001b[39m(input_shape\u001b[38;5;241m=\u001b[39m(\u001b[43mIMG_SIZE\u001b[49m, IMG_SIZE, \u001b[38;5;241m3\u001b[39m)):\n\u001b[0;32m      3\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39minput_shape)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Encoder\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'IMG_SIZE' is not defined"
     ]
    }
   ],
   "source": [
    "# 🧠 Step 4: Build U-Net Model\n",
    "def unet_model(input_shape=(IMG_SIZE, IMG_SIZE, 3)):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    c1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    c1 = layers.Conv2D(64, 3, activation='relu', padding='same')(c1)\n",
    "    p1 = layers.MaxPooling2D()(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(128, 3, activation='relu', padding='same')(p1)\n",
    "    c2 = layers.Conv2D(128, 3, activation='relu', padding='same')(c2)\n",
    "    p2 = layers.MaxPooling2D()(c2)\n",
    "\n",
    "    c3 = layers.Conv2D(256, 3, activation='relu', padding='same')(p2)\n",
    "    c3 = layers.Conv2D(256, 3, activation='relu', padding='same')(c3)\n",
    "    p3 = layers.MaxPooling2D()(c3)\n",
    "\n",
    "    # Bottleneck\n",
    "    c4 = layers.Conv2D(512, 3, activation='relu', padding='same')(p3)\n",
    "    c4 = layers.Conv2D(512, 3, activation='relu', padding='same')(c4)\n",
    "\n",
    "    # Decoder\n",
    "    u5 = layers.Conv2DTranspose(256, 2, strides=2, padding='same')(c4)\n",
    "    u5 = layers.concatenate([u5, c3])\n",
    "    c5 = layers.Conv2D(256, 3, activation='relu', padding='same')(u5)\n",
    "\n",
    "    u6 = layers.Conv2DTranspose(128, 2, strides=2, padding='same')(c5)\n",
    "    u6 = layers.concatenate([u6, c2])\n",
    "    c6 = layers.Conv2D(128, 3, activation='relu', padding='same')(u6)\n",
    "\n",
    "    u7 = layers.Conv2DTranspose(64, 2, strides=2, padding='same')(c6)\n",
    "    u7 = layers.concatenate([u7, c1])\n",
    "    c7 = layers.Conv2D(64, 3, activation='relu', padding='same')(u7)\n",
    "\n",
    "    outputs = layers.Conv2D(1, 1, activation='sigmoid')(c7)\n",
    "    return models.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b0fd14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
