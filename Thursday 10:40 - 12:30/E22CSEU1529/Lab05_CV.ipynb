{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dl1Fu3ah08BF",
        "outputId": "cd72e7a6-d9f0-49fd-eab8-87335fc14606"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DCT compressed JPEG image saved at compressed_img.jpg\n",
            "DCT compressed PNG image saved at compressed_img.png\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import mnist, cifar10\n",
        "\n",
        "# Task 1: Image Compression\n",
        "\n",
        "\n",
        "def compress_image(input_path, output_path, quality, format):\n",
        "    # Read the image\n",
        "    img = cv2.imread(input_path)\n",
        "    if img is None:\n",
        "        print(\"Error: Could not load image!\")\n",
        "        return\n",
        "\n",
        "    img = np.float32(img)  # Keep pixel range in [0, 255] but float for DCT\n",
        "\n",
        "    if format == 'JPEG':\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)  # Convert to YCrCb\n",
        "        Y, Cr, Cb = cv2.split(img)  # Separate channels\n",
        "    else:\n",
        "        # PNG (Apply DCT separately on R, G, and B channels)\n",
        "        Y, Cr, Cb = cv2.split(img)\n",
        "\n",
        "    # Define JPEG Quantization Matrix (scaled based on quality)\n",
        "    Q = np.array([\n",
        "        [16, 11, 10, 16, 24, 40, 51, 61],\n",
        "        [12, 12, 14, 19, 26, 58, 60, 55],\n",
        "        [14, 13, 16, 24, 40, 57, 69, 56],\n",
        "        [14, 17, 22, 29, 51, 87, 80, 62],\n",
        "        [18, 22, 37, 56, 68, 109, 103, 77],\n",
        "        [24, 35, 55, 64, 81, 104, 113, 92],\n",
        "        [49, 64, 78, 87, 103, 121, 120, 101],\n",
        "        [72, 92, 95, 98, 112, 100, 103, 99]\n",
        "    ]) * (100 - quality) / 50  # Scale based on quality\n",
        "\n",
        "    def dct_compress_channel(channel):\n",
        "        \"\"\" Applies DCT compression block-wise on a single channel \"\"\"\n",
        "        h, w = channel.shape\n",
        "        h_pad = 8 - (h % 8) if h % 8 != 0 else 0\n",
        "        w_pad = 8 - (w % 8) if w % 8 != 0 else 0\n",
        "\n",
        "        channel_padded = np.pad(channel, ((0, h_pad), (0, w_pad)), mode='constant')\n",
        "        h_padded, w_padded = channel_padded.shape\n",
        "        compressed_channel = np.zeros_like(channel_padded)\n",
        "\n",
        "        for y in range(0, h_padded, 8):\n",
        "            for x in range(0, w_padded, 8):\n",
        "                block = channel_padded[y:y+8, x:x+8] - 128  # Shift for DCT\n",
        "                dct_block = cv2.dct(block)\n",
        "                quantized_block = np.round(dct_block / Q) * Q\n",
        "                idct_block = cv2.idct(quantized_block) + 128  # Shift back\n",
        "                compressed_channel[y:y+8, x:x+8] = idct_block\n",
        "\n",
        "        return compressed_channel[:h, :w]  # Remove padding\n",
        "\n",
        "    # Apply DCT compression\n",
        "    Y_compressed = dct_compress_channel(Y)\n",
        "    if format == 'JPEG':\n",
        "        # JPEG: Keep Cr and Cb unchanged\n",
        "        img_compressed = cv2.merge([Y_compressed, Cr, Cb])\n",
        "        img_compressed = cv2.cvtColor(img_compressed, cv2.COLOR_YCrCb2BGR)  # Convert back to BGR\n",
        "    else:\n",
        "        # PNG: Apply DCT to all channels\n",
        "        Cr_compressed = dct_compress_channel(Cr)\n",
        "        Cb_compressed = dct_compress_channel(Cb)\n",
        "        img_compressed = cv2.merge([Y_compressed, Cr_compressed, Cb_compressed])\n",
        "\n",
        "    # Normalize back to 0-255\n",
        "    img_compressed = np.clip(img_compressed, 0, 255).astype(np.uint8)\n",
        "\n",
        "    # Save compressed image\n",
        "    if format == 'JPEG':\n",
        "        cv2.imwrite(output_path, img_compressed, [int(cv2.IMWRITE_JPEG_QUALITY), quality])\n",
        "    elif format == 'PNG':\n",
        "        cv2.imwrite(output_path, img_compressed, [int(cv2.IMWRITE_PNG_COMPRESSION), 9 - quality // 10])\n",
        "\n",
        "    print(f\"DCT compressed {format} image saved at {output_path}\")\n",
        "\n",
        "input_image=\"test.jpg\"\n",
        "op_path_jpeg=\"compressed_img.jpg\"\n",
        "op_path_png=\"compressed_img.png\"\n",
        "jpeg_quality=50\n",
        "png_compression=50\n",
        "\n",
        "compress_image(input_image, op_path_jpeg, jpeg_quality, \"JPEG\")\n",
        "\n",
        "compress_image(input_image, op_path_png, png_compression, \"PNG\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load MNIST dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocessing MNIST\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "\n",
        "# Build CNN Model for MNIST\n",
        "def build_mnist_cnn():\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Conv2D(64, (3,3), activation='relu'),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train MNIST Model\n",
        "mnist_model = build_mnist_cnn()\n",
        "mnist_model.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Evaluate MNIST Model\n",
        "y_pred_mnist = mnist_model.predict(x_test)\n",
        "y_pred_classes_mnist = np.argmax(y_pred_mnist, axis=1)\n",
        "y_true_mnist = np.argmax(y_test, axis=1)\n",
        "print(\"MNIST Classification Report:\")\n",
        "print(classification_report(y_true_mnist, y_pred_classes_mnist))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_true_mnist, y_pred_classes_mnist))\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "cifar10 = tf.keras.datasets.cifar10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Preprocessing CIFAR-10\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Build CNN Model for CIFAR-10\n",
        "def build_cifar10_cnn():\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Conv2D(64, (3,3), activation='relu'),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Conv2D(128, (3,3), activation='relu'),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train CIFAR-10 Model\n",
        "cifar10_model = build_cifar10_cnn()\n",
        "cifar10_model.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Evaluate CIFAR-10 Model\n",
        "y_pred_cifar10 = cifar10_model.predict(x_test)\n",
        "y_pred_classes_cifar10 = np.argmax(y_pred_cifar10, axis=1)\n",
        "y_true_cifar10 = np.argmax(y_test, axis=1)\n",
        "print(\"CIFAR-10 Classification Report:\")\n",
        "print(classification_report(y_true_cifar10, y_pred_classes_cifar10))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_true_cifar10, y_pred_classes_cifar10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN2VVbg217NX",
        "outputId": "9f1c4b96-49ec-47c2-9a77-f1345e839c2f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 62ms/step - accuracy: 0.8750 - loss: 0.4209 - val_accuracy: 0.9808 - val_loss: 0.0656\n",
            "Epoch 2/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 59ms/step - accuracy: 0.9808 - loss: 0.0624 - val_accuracy: 0.9840 - val_loss: 0.0530\n",
            "Epoch 3/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 57ms/step - accuracy: 0.9887 - loss: 0.0362 - val_accuracy: 0.9881 - val_loss: 0.0410\n",
            "Epoch 4/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 56ms/step - accuracy: 0.9918 - loss: 0.0258 - val_accuracy: 0.9884 - val_loss: 0.0391\n",
            "Epoch 5/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 55ms/step - accuracy: 0.9928 - loss: 0.0216 - val_accuracy: 0.9886 - val_loss: 0.0395\n",
            "Epoch 6/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 56ms/step - accuracy: 0.9947 - loss: 0.0170 - val_accuracy: 0.9875 - val_loss: 0.0426\n",
            "Epoch 7/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 58ms/step - accuracy: 0.9967 - loss: 0.0105 - val_accuracy: 0.9898 - val_loss: 0.0360\n",
            "Epoch 8/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 57ms/step - accuracy: 0.9965 - loss: 0.0098 - val_accuracy: 0.9895 - val_loss: 0.0391\n",
            "Epoch 9/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 56ms/step - accuracy: 0.9976 - loss: 0.0074 - val_accuracy: 0.9883 - val_loss: 0.0430\n",
            "Epoch 10/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 56ms/step - accuracy: 0.9982 - loss: 0.0056 - val_accuracy: 0.9879 - val_loss: 0.0466\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step\n",
            "MNIST Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      0.99       980\n",
            "           1       1.00      0.99      1.00      1135\n",
            "           2       0.99      0.99      0.99      1032\n",
            "           3       0.99      0.99      0.99      1010\n",
            "           4       1.00      0.99      0.99       982\n",
            "           5       0.98      0.99      0.99       892\n",
            "           6       1.00      0.99      0.99       958\n",
            "           7       0.98      1.00      0.99      1028\n",
            "           8       1.00      0.98      0.99       974\n",
            "           9       0.98      0.99      0.99      1009\n",
            "\n",
            "    accuracy                           0.99     10000\n",
            "   macro avg       0.99      0.99      0.99     10000\n",
            "weighted avg       0.99      0.99      0.99     10000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 973    1    1    0    0    1    1    1    1    1]\n",
            " [   0 1129    1    0    1    1    1    2    0    0]\n",
            " [   0    1 1022    0    2    0    0    7    0    0]\n",
            " [   0    0    1 1003    0    5    0    0    1    0]\n",
            " [   0    0    0    0  973    0    0    0    0    9]\n",
            " [   0    0    1    8    0  881    1    0    1    0]\n",
            " [   3    2    0    1    0    4  946    0    1    1]\n",
            " [   0    0    1    0    0    0    0 1023    0    4]\n",
            " [   1    0    5    3    0    1    0    3  958    3]\n",
            " [   0    0    0    0    0    3    0    4    0 1002]]\n",
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
            "Epoch 1/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 99ms/step - accuracy: 0.3052 - loss: 1.8612 - val_accuracy: 0.4815 - val_loss: 1.4171\n",
            "Epoch 2/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 94ms/step - accuracy: 0.5508 - loss: 1.2616 - val_accuracy: 0.6005 - val_loss: 1.1368\n",
            "Epoch 3/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 94ms/step - accuracy: 0.6185 - loss: 1.0750 - val_accuracy: 0.6223 - val_loss: 1.0730\n",
            "Epoch 4/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 100ms/step - accuracy: 0.6650 - loss: 0.9606 - val_accuracy: 0.6674 - val_loss: 0.9711\n",
            "Epoch 5/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 94ms/step - accuracy: 0.6981 - loss: 0.8573 - val_accuracy: 0.6894 - val_loss: 0.9148\n",
            "Epoch 6/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 90ms/step - accuracy: 0.7292 - loss: 0.7807 - val_accuracy: 0.6903 - val_loss: 0.9123\n",
            "Epoch 7/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 93ms/step - accuracy: 0.7529 - loss: 0.7129 - val_accuracy: 0.6957 - val_loss: 0.8946\n",
            "Epoch 8/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 92ms/step - accuracy: 0.7703 - loss: 0.6542 - val_accuracy: 0.7047 - val_loss: 0.8855\n",
            "Epoch 9/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 119ms/step - accuracy: 0.7938 - loss: 0.5929 - val_accuracy: 0.7066 - val_loss: 0.8999\n",
            "Epoch 10/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 97ms/step - accuracy: 0.8108 - loss: 0.5468 - val_accuracy: 0.7032 - val_loss: 0.9124\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step\n",
            "CIFAR-10 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.80      0.72      1000\n",
            "           1       0.87      0.78      0.82      1000\n",
            "           2       0.58      0.66      0.62      1000\n",
            "           3       0.54      0.51      0.52      1000\n",
            "           4       0.77      0.53      0.62      1000\n",
            "           5       0.56      0.70      0.62      1000\n",
            "           6       0.85      0.72      0.78      1000\n",
            "           7       0.70      0.77      0.73      1000\n",
            "           8       0.86      0.76      0.81      1000\n",
            "           9       0.78      0.80      0.79      1000\n",
            "\n",
            "    accuracy                           0.70     10000\n",
            "   macro avg       0.72      0.70      0.70     10000\n",
            "weighted avg       0.72      0.70      0.70     10000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[802  15  51  16   7  13   5  11  46  34]\n",
            " [ 44 782  13  10   1  13  10   9  15 103]\n",
            " [ 88   4 655  54  55  70  31  31   5   7]\n",
            " [ 37   2  97 510  29 225  27  52   7  14]\n",
            " [ 35   3  85  94 526  87  33 122  10   5]\n",
            " [ 18   2  62 120  19 703  10  56   5   5]\n",
            " [ 15   4  73  85  20  46 720  23   7   7]\n",
            " [ 22   1  47  35  25  78   3 770   4  15]\n",
            " [121  24  24  13   2  11   3   7 760  35]\n",
            " [ 55  59  15  13   1  14   4  16  26 797]]\n"
          ]
        }
      ]
    }
  ]
}