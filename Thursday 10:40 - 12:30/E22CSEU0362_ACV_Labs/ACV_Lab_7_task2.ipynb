{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jn-U6vJLh4S5","outputId":"1ef18fc5-6c0c-42bf-e774-4ae528b41e5d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 0.0000\n","Epoch 2, Loss: 0.0000\n","Epoch 3, Loss: 0.0000\n","Epoch 4, Loss: 0.0000\n","Epoch 5, Loss: 0.0000\n","Accuracy: 64.14%, Inference Time: 17.65s\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n","100%|██████████| 528M/528M [00:07<00:00, 76.6MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 0.0000\n","Epoch 2, Loss: 0.0000\n","Epoch 3, Loss: 0.0000\n","Epoch 4, Loss: 0.0000\n","Epoch 5, Loss: 0.0000\n","Accuracy: 70.23%, Inference Time: 57.84s\n","AlexNet - Accuracy: 64.14%, Time: 17.65s\n","VGG16 - Accuracy: 70.23%, Time: 57.84s\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","import time\n","from torchvision.models import vgg16, alexnet\n","from torch.utils.data import DataLoader\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Enhanced Data Augmentation\n","transform = transforms.Compose([\n","    transforms.RandomHorizontalFlip(),  # Helps generalization\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n","])\n","\n","train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n","test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n","\n","\n","def get_model(model_name):\n","    if model_name == \"alexnet\":\n","        model = alexnet(pretrained=True)\n","        for param in model.features.parameters():\n","            param.requires_grad = True\n","\n","        model.classifier = nn.Sequential(\n","            nn.Linear(9216, 4096),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(4096, 100)\n","        )\n","    elif model_name == \"vgg16\":\n","        model = vgg16(pretrained=True)\n","        for param in model.features.parameters():\n","            param.requires_grad = True\n","        model.classifier[6] = nn.Linear(4096, 100)\n","\n","    return model.to(device)\n","\n","# Training Function with Gradient Clipping\n","def train_model(model, train_loader, criterion, optimizer, scheduler, epochs=5):\n","    model.train()\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Avoid exploding gradients\n","            optimizer.step()\n","        scheduler.step()\n","        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")\n","\n","# Evaluation Function\n","def evaluate_model(model, test_loader):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    start_time = time.time()\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    end_time = time.time()\n","    accuracy = 100 * correct / total\n","    print(f\"Accuracy: {accuracy:.2f}%, Inference Time: {end_time - start_time:.2f}s\")\n","    return accuracy, end_time - start_time\n","\n","\n","alexnet_model = get_model(\"alexnet\")\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(alexnet_model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)  # Better generalization\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.7)  # Learning rate decay\n","train_model(alexnet_model, train_loader, criterion, optimizer, scheduler, epochs=5)\n","alexnet_acc, alexnet_time = evaluate_model(alexnet_model, test_loader)\n","\n","\n","vgg16_model = get_model(\"vgg16\")\n","optimizer = optim.SGD(vgg16_model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.7)\n","train_model(vgg16_model, train_loader, criterion, optimizer, scheduler, epochs=5)\n","vgg16_acc, vgg16_time = evaluate_model(vgg16_model, test_loader)\n","\n","# Compare Results\n","print(f\"AlexNet - Accuracy: {alexnet_acc:.2f}%, Time: {alexnet_time:.2f}s\")\n","print(f\"VGG16 - Accuracy: {vgg16_acc:.2f}%, Time: {vgg16_time:.2f}s\")\n"]}]}